{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Perturbation with 5,10 and 100% rate**"
      ],
      "metadata": {
        "id": "O6zPb6VoJ--T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27IYPgYsJ60k"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "\n",
        "def perturbation_keep_clustering(sn_graph, prob_new_connection, prob_remove_connection, max_candidates=50):\n",
        "    ins_connection = []\n",
        "    del_connection = []\n",
        "\n",
        "    for node in sn_graph.nodes:\n",
        "        # Get neighbors and sample non-neighbors\n",
        "        neighbors = set(sn_graph.neighbors(node))\n",
        "        non_neighbors = [\n",
        "            n for n in sn_graph.nodes if n not in neighbors and n != node\n",
        "        ]\n",
        "        non_neighbors_sample = random.sample(non_neighbors, min(len(non_neighbors), max_candidates))\n",
        "\n",
        "        # Add edges to form new triangles\n",
        "        if non_neighbors_sample and random.random() < prob_new_connection:\n",
        "            for n in non_neighbors_sample:\n",
        "                # Check if adding this edge forms a triangle\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 0:\n",
        "                    sn_graph.add_edge(node, n)\n",
        "                    ins_connection.append((node, n))\n",
        "                    break  # Add only one edge per node in this iteration\n",
        "\n",
        "        # Sample neighbors for edge removal\n",
        "        neighbors_sample = random.sample(neighbors, min(len(neighbors), max_candidates))\n",
        "        if neighbors_sample and random.random() < prob_remove_connection:\n",
        "            for n in neighbors_sample:\n",
        "                # Check if removing this edge preserves clustering\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 1:  # Removing this edge won't break triangles\n",
        "                    sn_graph.remove_edge(node, n)\n",
        "                    del_connection.append((node, n))\n",
        "                    break  # Remove only one edge per node in this iteration\n",
        "\n",
        "    return del_connection, ins_connection\n",
        "\n",
        "def run_algo_with_different_perturbations(dataset_path):\n",
        "    dataset = nx.read_gml(dataset_path)\n",
        "    display_info(dataset)\n",
        "\n",
        "    dataset_naive = nx.convert_node_labels_to_integers(dataset, first_label=1, ordering='default')\n",
        "    original_clustering = nx.average_clustering(dataset_naive)\n",
        "\n",
        "    perturbation_levels = [0.05, 0.1, 1.0]  # 5%, 10%, and 100% perturbation\n",
        "    for prob in perturbation_levels:\n",
        "        dataset_new = dataset_naive.copy()\n",
        "        del_connection, new_connection = perturbation_keep_clustering(\n",
        "            dataset_new, prob, prob\n",
        "        )\n",
        "\n",
        "        # Calculate and display clustering coefficients\n",
        "        clustering_coeff_original = original_clustering\n",
        "        clustering_coeff_new = nx.average_clustering(dataset_new)\n",
        "        print(f\"\\n=== {int(prob * 100)}% Perturbation ===\")\n",
        "        print(f\"Original Clustering Coefficient: {clustering_coeff_original}\")\n",
        "        print(f\"New Clustering Coefficient: {clustering_coeff_new}\")\n",
        "        print(\"Graph changes:\")\n",
        "        print(f\"  Edges added: {len(new_connection)}\")\n",
        "        print(f\"  Edges removed: {len(del_connection)}\")\n",
        "        display_info(dataset_new)\n",
        "\n",
        "def display_info(sn_graph):\n",
        "\n",
        "    try:\n",
        "        print(nx.info(sn_graph))\n",
        "        betweenness = nx.betweenness_centrality(sn_graph, normalized=True, k=50)  # Reduce k for speed\n",
        "        print(\"Betweenness centrality (average):\", sum(betweenness.values()) / len(betweenness))\n",
        "        closeness = nx.closeness_centrality(sn_graph)\n",
        "        print(\"Closeness centrality (average):\", sum(closeness.values()) / len(closeness))\n",
        "        print(\"Number of isolates:\", nx.number_of_isolates(sn_graph))\n",
        "    except Exception as e:\n",
        "        print(\"Error measuring graph properties:\", e)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    # Provide the path to your GML dataset\n",
        "    dataset_path = 'soc-Epinions1-converted.gml'  # Replace with different datasets\n",
        "    run_algo_with_different_perturbations(dataset_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualisation after and before 10 % of pertyrbation on Wiki-vote network**"
      ],
      "metadata": {
        "id": "hv0CDR4_KJHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "\n",
        "def visualize_triangles(graph, max_nodes=100, title=\"Graph Visualization Highlighting Triangles\"):\n",
        "\n",
        "    if graph.number_of_nodes() == 0:\n",
        "        print(\"The graph is empty. Nothing to visualize.\")\n",
        "        return\n",
        "\n",
        "    # Limit to a subgraph if the graph is too large\n",
        "    if len(graph) > max_nodes:\n",
        "        subgraph = graph.subgraph(list(graph.nodes)[:max_nodes])\n",
        "        print(f\"Graph is large, visualizing first {max_nodes} nodes.\")\n",
        "    else:\n",
        "        subgraph = graph\n",
        "\n",
        "    # Identify triangles in the graph\n",
        "    triangle_edges = set()\n",
        "    for node in subgraph.nodes:\n",
        "        neighbors = set(subgraph.neighbors(node))\n",
        "        for pair in combinations(neighbors, 2):\n",
        "            if subgraph.has_edge(pair[0], pair[1]):  # Check if a triangle exists\n",
        "                triangle_edges.update([(node, pair[0]), (node, pair[1]), (pair[0], pair[1])])\n",
        "\n",
        "    pos = nx.spring_layout(subgraph)  # Spring layout\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    nx.draw_networkx_nodes(subgraph, pos, node_color=\"lightblue\", node_size=500, alpha=0.8)\n",
        "    nx.draw_networkx_edges(subgraph, pos, edge_color=\"gray\", alpha=0.5)\n",
        "    nx.draw_networkx_labels(subgraph, pos, font_size=10)\n",
        "\n",
        "    nx.draw_networkx_edges(\n",
        "        subgraph,\n",
        "        pos,\n",
        "        edgelist=list(triangle_edges),\n",
        "        edge_color=\"red\",\n",
        "        width=2,\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def perturbation_10_percent_keep_clustering(sn_graph, prob_new_connection, prob_remove_connection, original_clustering, max_candidates=50):\n",
        "\n",
        "    ins_connection = []\n",
        "    del_connection = []\n",
        "\n",
        "    for node in sn_graph.nodes:\n",
        "        neighbors = set(sn_graph.neighbors(node))\n",
        "        non_neighbors = [n for n in sn_graph.nodes if n not in neighbors and n != node]\n",
        "        non_neighbors_sample = random.sample(non_neighbors, min(len(non_neighbors), max_candidates))\n",
        "\n",
        "        # Add edges to form new triangles\n",
        "        if non_neighbors_sample and random.random() < prob_new_connection:\n",
        "            for n in non_neighbors_sample:\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 0:\n",
        "                    sn_graph.add_edge(node, n)\n",
        "                    ins_connection.append((node, n))\n",
        "                    break\n",
        "\n",
        "        # Sample neighbors for edge removal\n",
        "        neighbors_sample = random.sample(neighbors, min(len(neighbors), max_candidates))\n",
        "        if neighbors_sample and random.random() < prob_remove_connection:\n",
        "            for n in neighbors_sample:\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 1:\n",
        "                    sn_graph.remove_edge(node, n)\n",
        "                    del_connection.append((node, n))\n",
        "                    break\n",
        "\n",
        "    return del_connection, ins_connection\n",
        "\n",
        "def run_algo_with_visualization_and_clustering(dataset_path):\n",
        "    dataset = nx.read_gml(dataset_path)\n",
        "    dataset_naive = nx.convert_node_labels_to_integers(dataset, first_label=1, ordering='default')\n",
        "    original_clustering = nx.average_clustering(dataset_naive)\n",
        "\n",
        "    # Visualize before perturbation\n",
        "    print(\"Visualizing BEFORE perturbation...\")\n",
        "    visualize_triangles(dataset_naive, max_nodes=100, title=\"Triangles Before Perturbation\")\n",
        "    print(f\"Clustering Coefficient BEFORE perturbation: {original_clustering:.6f}\")\n",
        "\n",
        "    # Perform perturbation\n",
        "    prob = 0.1  # 10% perturbation\n",
        "    dataset_new = dataset_naive.copy()\n",
        "    perturbation_10_percent_keep_clustering(\n",
        "        dataset_new, prob, prob, original_clustering\n",
        "    )\n",
        "\n",
        "    # Visualize after perturbation\n",
        "    print(\"Visualizing AFTER perturbation...\")\n",
        "    visualize_triangles(dataset_new, max_nodes=100, title=\"Triangles After Perturbation\")\n",
        "    new_clustering = nx.average_clustering(dataset_new)\n",
        "    print(f\"Clustering Coefficient AFTER perturbation: {new_clustering:.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Provide the path to your GML dataset\n",
        "    dataset_path = 'wiki-Vote.gml'\n",
        "    run_algo_with_visualization_and_clustering(dataset_path)\n"
      ],
      "metadata": {
        "id": "gXOl7MiQKPz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entropy of clustering and diameter of graph**"
      ],
      "metadata": {
        "id": "MLF-2uZxKSPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "from networkx.algorithms import approximation\n",
        "\n",
        "def compute_entropy(attribute_values):\n",
        "    \"\"\"\n",
        "    Compute the entropy of a given attribute distribution.\n",
        "    Higher entropy means less variability (better anonymity).\n",
        "    \"\"\"\n",
        "    unique, counts = np.unique(attribute_values, return_counts=True)\n",
        "    probabilities = counts / len(attribute_values)\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy\n",
        "\n",
        "def check_anonymity_and_diameter(graph, sample_threshold=1000):\n",
        "\n",
        "    # Compute clustering coefficient\n",
        "\n",
        "    clustering_coeffs = [nx.clustering(graph, n) for n in graph.nodes()]\n",
        "\n",
        "    # Calculate entropy for anonymity\n",
        "    degree_entropy = compute_entropy(degrees)\n",
        "    clustering_entropy = compute_entropy(clustering_coeffs)\n",
        "\n",
        "    # Calculate diameter\n",
        "    if nx.is_connected(graph):\n",
        "        if len(graph) > sample_threshold:\n",
        "            # Approximate diameter for large graphs\n",
        "            diameter = approximation.diameter(graph)\n",
        "        else:\n",
        "            diameter = nx.diameter(graph)\n",
        "    else:\n",
        "        largest_cc = max(nx.connected_components(graph), key=len)\n",
        "        largest_cc_subgraph = graph.subgraph(largest_cc)\n",
        "\n",
        "        if len(largest_cc_subgraph) > sample_threshold:\n",
        "            # Approximate diameter for large connected component\n",
        "            diameter = approximation.diameter(largest_cc_subgraph)\n",
        "        else:\n",
        "            diameter = nx.diameter(largest_cc_subgraph)\n",
        "\n",
        "    print(f\"Clustering Coefficient Entropy: {clustering_entropy:.4f}\")\n",
        "    print(f\"Graph Diameter: {diameter}\")\n",
        "\n",
        "    return degree_entropy, clustering_entropy, diameter\n",
        "\n",
        "def perturbation_keep_clustering(sn_graph, prob_new_connection, prob_remove_connection, max_candidates=50):\n",
        "\n",
        "    ins_connection = []\n",
        "    del_connection = []\n",
        "\n",
        "    for node in sn_graph.nodes:\n",
        "        neighbors = set(sn_graph.neighbors(node))\n",
        "        non_neighbors = [n for n in sn_graph.nodes if n not in neighbors and n != node]\n",
        "        non_neighbors_sample = random.sample(non_neighbors, min(len(non_neighbors), max_candidates))\n",
        "\n",
        "        # Add edges to form new triangles\n",
        "        if non_neighbors_sample and random.random() < prob_new_connection:\n",
        "            for n in non_neighbors_sample:\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 0:\n",
        "                    sn_graph.add_edge(node, n)\n",
        "                    ins_connection.append((node, n))\n",
        "                    break\n",
        "\n",
        "        # Sample neighbors for edge removal\n",
        "        neighbors_sample = random.sample(neighbors, min(len(neighbors), max_candidates))\n",
        "        if neighbors_sample and random.random() < prob_remove_connection:\n",
        "            for n in neighbors_sample:\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 1:\n",
        "                    sn_graph.remove_edge(node, n)\n",
        "                    del_connection.append((node, n))\n",
        "                    break\n",
        "\n",
        "    return del_connection, ins_connection\n",
        "\n",
        "def run_analysis_with_perturbations(dataset_path):\n",
        "    # Load the original graph\n",
        "    original_graph = nx.read_gml(dataset_path)\n",
        "\n",
        "    # Convert to undirected for diameter calculation\n",
        "    undirected_graph = original_graph.to_undirected()\n",
        "\n",
        "    dataset_naive = nx.convert_node_labels_to_integers(undirected_graph, first_label=1, ordering='default')\n",
        "\n",
        "    # Original clustering coefficient\n",
        "    original_clustering = nx.average_clustering(dataset_naive)\n",
        "\n",
        "    # Check anonymity and diameter before perturbation\n",
        "    print(\"=== Before Perturbation ===\")\n",
        "    print(f\"Clustering Coefficient BEFORE: {original_clustering:.4f}\")\n",
        "    entropy_before, clustering_entropy_before, diameter_before = check_anonymity_and_diameter(dataset_naive)\n",
        "\n",
        "    # perturbations\n",
        "    perturbation_levels = [0.05, 0.1, 1.0]  # 5%, 10%, and 100% perturbation\n",
        "    for prob in perturbation_levels:\n",
        "        perturbed_graph = dataset_naive.copy()\n",
        "        perturbation_keep_clustering(\n",
        "            perturbed_graph, prob_new_connection=prob, prob_remove_connection=prob\n",
        "        )\n",
        "\n",
        "        # New clustering coefficient\n",
        "        new_clustering = nx.average_clustering(perturbed_graph)\n",
        "\n",
        "        # Check entropy and diameter after perturbation\n",
        "        print(f\"\\n=== After {int(prob * 100)}% Perturbation ===\")\n",
        "        print(f\"Clustering Coefficient AFTER: {new_clustering:.4f}\")\n",
        "       clustering_entropy_after, diameter_after = check_anonymity_and_diameter(perturbed_graph)\n",
        "\n",
        "        print(f\"\\nSummary for {int(prob * 100)}% Perturbation:\")\n",
        "        print(f\"Clustering Entropy Change: {clustering_entropy_before:.4f} → {clustering_entropy_after:.4f}\")\n",
        "        print(f\"Diameter Change: {diameter_before} → {diameter_after}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"soc-Epinions1-converted.gml\"  # Replace with different datasets\n",
        "    run_analysis_with_perturbations(dataset_path)\n"
      ],
      "metadata": {
        "id": "R6bbFEfqKWxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NMI SCORE**"
      ],
      "metadata": {
        "id": "XTAW7xXMKZt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import random\n",
        "import community as community_louvain\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def perturbation_keep_clustering(sn_graph, prob_new_connection, prob_remove_connection, max_candidates=50):\n",
        "\n",
        "    for node in sn_graph.nodes:\n",
        "        neighbors = set(sn_graph.neighbors(node))\n",
        "        non_neighbors = [n for n in sn_graph.nodes if n not in neighbors and n != node]\n",
        "        non_neighbors_sample = random.sample(non_neighbors, min(len(non_neighbors), max_candidates))\n",
        "\n",
        "        # Add edges to form new triangles\n",
        "        if non_neighbors_sample and random.random() < prob_new_connection:\n",
        "            for n in non_neighbors_sample:\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 0:\n",
        "                    sn_graph.add_edge(node, n)\n",
        "                    break\n",
        "\n",
        "        # Sample neighbors for edge removal\n",
        "        neighbors_sample = random.sample(neighbors, min(len(neighbors), max_candidates))\n",
        "        if neighbors_sample and random.random() < prob_remove_connection:\n",
        "            for n in neighbors_sample:\n",
        "                shared_neighbors = neighbors & set(sn_graph.neighbors(n))\n",
        "                if len(shared_neighbors) > 1:\n",
        "                    sn_graph.remove_edge(node, n)\n",
        "                    break\n",
        "\n",
        "\n",
        "def detect_communities(graph):\n",
        "\n",
        "    return community_louvain.best_partition(graph)\n",
        "\n",
        "\n",
        "def compare_communities(partition1, partition2):\n",
        "\n",
        "    labels1 = list(partition1.values())\n",
        "    labels2 = list(partition2.values())\n",
        "    return normalized_mutual_info_score(labels1, labels2)\n",
        "\n",
        "\n",
        "def visualize_communities(graph, partition, title=\"Community Structure\"):\n",
        "\n",
        "    pos = nx.spring_layout(graph)\n",
        "    communities = set(partition.values())\n",
        "    for community in communities:\n",
        "        nodes = [node for node in partition if partition[node] == community]\n",
        "        nx.draw_networkx_nodes(graph, pos, nodelist=nodes, node_size=50, label=f\"Community {community}\")\n",
        "    nx.draw_networkx_edges(graph, pos, alpha=0.5)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def run_community_analysis(dataset_path):\n",
        "    # Load the original graph\n",
        "    original_graph = nx.read_gml(dataset_path)\n",
        "\n",
        "    # Convert to undirected for community detection\n",
        "    graph = original_graph.to_undirected()\n",
        "\n",
        "    # Detect communities in the original graph\n",
        "    original_partition = detect_communities(graph)\n",
        "    print(\"=== Communities Detected in Original Graph ===\")\n",
        "    visualize_communities(graph, original_partition, title=\"Communities Before Perturbation\")\n",
        "\n",
        "    # Perform perturbation\n",
        "    perturbation_levels = [0.05, 0.1, 1.0]  # 5%, 10%, and 100% perturbation\n",
        "    for prob in perturbation_levels:\n",
        "        perturbed_graph = graph.copy()\n",
        "        perturbation_keep_clustering(perturbed_graph, prob_new_connection=prob, prob_remove_connection=prob)\n",
        "\n",
        "        # Detect communities in the perturbed graph\n",
        "        perturbed_partition = detect_communities(perturbed_graph)\n",
        "        print(f\"\\n=== Communities Detected After {int(prob * 100)}% Perturbation ===\")\n",
        "        visualize_communities(perturbed_graph, perturbed_partition, title=f\"Communities After {int(prob * 100)}% Perturbation\")\n",
        "\n",
        "        # Compare communities using NMI\n",
        "        nmi_score = compare_communities(original_partition, perturbed_partition)\n",
        "        print(f\"NMI Score for {int(prob * 100)}% Perturbation: {nmi_score:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"wiki-Vote.gml\"  # Replace with different datasets\n",
        "    run_community_analysis(dataset_path)\n"
      ],
      "metadata": {
        "id": "wJym1ia2Kb5o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}